{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/AI_Image_Detector'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI_Image_Detector/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/AI_Image_Detector/venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionXLPipeline {\n",
       "  \"_class_name\": \"StableDiffusionXLPipeline\",\n",
       "  \"_diffusers_version\": \"0.26.2\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"force_zeros_for_empty_prompt\": true,\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"cuda\")\n",
    "\n",
    "# refiner = DiffusionPipeline.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "#     text_encoder_2=base.text_encoder_2,\n",
    "#     vae=base.vae,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_safetensors=True,\n",
    "#     variant=\"fp16\",\n",
    "# )\n",
    "# refiner.to(\"cuda\")\n",
    "\n",
    "# # Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "# n_steps = 40\n",
    "# high_noise_frac = 0.8\n",
    "\n",
    "# # prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "# prompt = \"an indian boy chased by his sister for stealing her lipstick\"\n",
    "\n",
    "# # run both experts\n",
    "# image = base(\n",
    "#     prompt=prompt,\n",
    "#     num_inference_steps=n_steps,\n",
    "#     denoising_end=high_noise_frac,\n",
    "#     output_type=\"latent\",\n",
    "# ).images\n",
    "# image = refiner(\n",
    "#     prompt=prompt,\n",
    "#     num_inference_steps=n_steps,\n",
    "#     denoising_start=high_noise_frac,\n",
    "#     image=image,\n",
    "# ).images[0]\n",
    "\n",
    "\n",
    "# image.save(\"stablity_ai_images/boy.jpg\")                                                                                                                                                                                   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"A serene mountain landscape with a cozy cabin nestled among pine trees.\"',\n",
       " '\"A futuristic cityscape with flying cars zooming between skyscrapers.\"',\n",
       " '\"An underwater scene with colorful coral reefs and exotic fish.\"']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts= []\n",
    "with open(\"research/prompts.json\") as f:\n",
    "    prompts= json.load(f)\n",
    "    \n",
    "prompts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:07<00:00,  3.00it/s]\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.52it/s]s/it]\n",
      "  0%|          | 2/20000 [00:18<52:20:50,  9.42s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "i=22\n",
    "for prompt in tqdm(prompts[i:20000]):\n",
    "    num_inference_steps= randint(20,40)\n",
    "    guidance_scale= randrange(0, 10)\n",
    "    \n",
    "    # Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "    n_steps = num_inference_steps\n",
    "    high_noise_frac = 0.8\n",
    "\n",
    "    # run both experts\n",
    "    image_i = base(prompt=prompt, num_inference_steps=n_steps).images[0]\n",
    "    image_i.save(f\"stablity_ai_images/image_{i}.JPEG\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "response= requests.get(\"https://playgroundai.com/search?q=zen+garden\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs  requirements.txt  research  scripts  stablity_ai_images  venv\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/playground_ai_garden.html\", \"w\") as f:\n",
    "    f.write(response.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
